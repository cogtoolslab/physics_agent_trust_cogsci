% 
% Annual Cognitive Science Conference
% Sample LaTeX Paper -- Proceedings Format
% 

% Original : Ashwin Ram (ashwin@cc.gatech.edu)       04/01/1994
% Modified : Johanna Moore (jmoore@cs.pitt.edu)      03/17/1995
% Modified : David Noelle (noelle@ucsd.edu)          03/15/1996
% Modified : Pat Langley (langley@cs.stanford.edu)   01/26/1997
% Latex2e corrections by Ramin Charles Nakisa        01/28/1997 
% Modified : Tina Eliassi-Rad (eliassi@cs.wisc.edu)  01/31/1998
% Modified : Trisha Yannuzzi (trisha@ircs.upenn.edu) 12/28/1999 (in process)
% Modified : Mary Ellen Foster (M.E.Foster@ed.ac.uk) 12/11/2000
% Modified : Ken Forbus                              01/23/2004
% Modified : Eli M. Silk (esilk@pitt.edu)            05/24/2005
% Modified : Niels Taatgen (taatgen@cmu.edu)         10/24/2006
% Modified : David Noelle (dnoelle@ucmerced.edu)     11/19/2014
% Modified : Roger Levy (rplevy@mit.edu)     12/31/2018



%% Change "letterpaper" in the following line to "a4paper" if you must.

\documentclass[10pt,letterpaper]{article}

\usepackage{cogsci}

% \cogscifinalcopy % Uncomment this line for the final submission 


\usepackage{pslatex}
\usepackage{apacite}
\usepackage{float} % Roger Levy added this and changed figure/table
                   % placement to [H] for conformity to Word template,
                   % though floating tables and figures to top is
                   % still generally recommended!

%\usepackage[none]{hyphenat} % Sometimes it can be useful to turn off
%hyphenation for purposes such as spell checking of the resulting
%PDF.  Uncomment this block to turn off hyphenation.


%\setlength\titlebox{4.5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 4.5cm (the original size).
%%If you do, we reserve the right to require you to change it back in
%%the camera-ready version, which could interfere with the timely
%%appearance of your paper in the Proceedings.

% erikb new packages
\usepackage{graphicx}
\usepackage{color}
\newcommand{\eb}[1]{{\color{blue}{\bf\sf [EB: #1]}}}


\title{Trust in learning agents}
 
\author{
  {
  % Erik
  \large \bf Erik Brockbank* (ebrockbank@ucsd.edu)} \\
  UCSD Department of Psychology, 9500 Gilman Drive \\
  La Jolla, CA 92093 USA
  % Haoliang
  \AND {\large \bf Haoliang Wang* (haw027@ucsd.edu)} \\
  UCSD Department of Psychology, 9500 Gilman Drive \\
  La Jolla, CA 92093 USA
  % Justin
  \AND {\large \bf Justin Yang (juy003@ucsd.edu)} \\
  UCSD Department of Psychology, 9500 Gilman Drive \\
  La Jolla, CA 92093 USA
  % Suvir
  \AND {\large \bf Suvir Mirchandani (suvir@cs.stanford.edu)} \\
  Stanford Department of Computer Science, 353 Jane Stanford Way \\
  Stanford, CA 94305 USA \\
  % Erdem
  \AND {\large \bf Erdem Biyik (ebiyik@stanford.edu)} \\
  Stanford Department of Computer Science, 353 Jane Stanford Way \\
  Stanford, CA 94305 USA \\
  % Dorsa
  \AND {\large \bf Dorsa Sadigh (dorsa@cs.stanford.edu)} \\
  Stanford Department of Computer Science, 353 Jane Stanford Way \\
  Stanford, CA 94305 USA \\
  % Judy
  \AND {\large \bf Judith Fan (jefan@ucsd.edu)} \\
  UCSD Department of Psychology, 9500 Gilman Drive \\
  La Jolla, CA 92093 USA \\
}


\begin{document}

\maketitle


\begin{abstract}
How do people build up trust with humans and artificial agents? In this work, we study a key component of interpersonal trust by investigating people's ability to evaluate the \textit{competence} of another agent across repeated interactions. Prior work has largely focused on static evaluations of trust and simple skills; in contrast, we probe competence evaluations in a rich physical setting with agents that learn over time. Participants played a video game testing their intuitive physical reasoning. They were paired with one of four artificial agents that suggested moves each round: an unreliable agent, a reliable agent, an improving agent, and a worsening agent. We measure participants' decisions to accept or revise their partner's suggestions to better understand how people determine another agent's competence from prior behavior. We find that people succeed with their agent partners in all cases but that decision making is not a matter of simply choosing the best option; people's behavior reflects the competence of their partner in earlier rounds. These results provide a quantitative measure of how people integrate a partner's competence into their own decisions and contributes to our understanding of how to better facilitate coordination between humans and artificial agents.

\textbf{Keywords:} 
trust; social inference; artificial agents; competence; learning 
\end{abstract}


\section{Introduction}

\textit{How do people build up trust across repeated interactions?} This question has motivated research from diverse areas of cognitive science spanning social psychology [CITE], game theory and economics [CITE], and as automated agents become increasingly ubiquitous in our everyday lives, robotics and human-computer interaction (HCI) [CITE]. 
In autonomous driving settings for instance, people routinely make decisions about how much to trust an artificial driving agent. And in many industrial domains, people work closely with automated agents, sometimes for high stakes tasks. The emergence of trust in our interactions with humans and artificial agents involves a range of complex social inferences, for example recognizing that one's collaborator shares one's goals or utilities to begin with [CITE]. However, one of the central features of human collaboration with artificial agents is that we trust them to be \textit{competent} across a range of task settings. Indeed, greater levels of trust may simply correspond to a belief that the agents are competent in a wider range of settings (e.g., greater trust in an autonomous vehicle may in large part reflect a belief that they can handle a wide array of driving environments and challenges).

Here we focus on the question of how people assess another agent's competence and how this affects downstream decisions about whether or not to trust them. Prior work in developmental psychology has shown that inferences about another person's competence based on their behavior emerge early in development and draw on a rich set of inputs. [EXAMPLES]. Broadly, this research suggests that when assessing another agent's competence, people bring a rich cognitive toolkit and a great deal of experience starting from a young age. Recent work in robotics and HCI suggests that when determining a robot or artificial agent's competence, people rely on similar cognitive processes. [EXAMPLES]. 

Despite this convergence of findings across developmental psychology and artificial intelligence, there remain a number of significant challenges in characterizing how people assess the competence of another agent or person. For one, real-world judgments of competence are often nebulous. How good is somebody at baking or predicting the stock market or writing academic papers? Second, in many complex settings, people's judgments of another agent's competence rely in large part on that agent's ability to \textit{learn} in the task environment. The current work builds on prior results by addressing both of these aspects of people's competence evaluations. First, in contrast with prior work which has largely focused on assessing concrete abilities, we embed people's competence judgments in a dynamic physical environment that allows for nuanced evaluations of competence. Second, rather than isolating competence judgments about static agents, the current experiment probes peoples' ability to detect another agent's \textit{learning} over time. In this way, a better understanding of how an agent's learning impacts people's competence judgments in rich physical settings may support a broader understanding of how people reason about the abilities of others, and how this reasoning impacts their subsequent decisions to trust them in a range of everyday settings.

Here, we investigate people's ability to collaborate with an artificial agent in a challenging physics-based video game. Participants work alongside their agent partner to try and catch a ball with a movable paddle. Building on prior work, we use people's decisions about whether or not to \textit{intervene} on their partner's behavior as an operationalization for how competent they judge their partner to be [CITES]. Critically, people's agent partners varied in their true competence and whether they were getting better at the task. We first ask how much people's behavior in the game draws on their own physical judgments versus the suggestions of their partner and how this varies based on their partner's ability or improvement. Next, we ask how much people's intervention decisions reflect nuanced, long range assessments of their partner's competence rather than trial-specific context. Our results suggest that people flexibly integrate their own physical knowledge with their partner's suggestions and succeed with partners that are learning and worsening over time. Further, we find evidence that people modulate the amount they trust their partner's suggestions based on an ongoing estimate of their partner's underlying competence; simple collaborative decisions involve rich and abstract inferences about others based on their past behavior.


Blah \cite{gweon2021inferential}.
See also \citeA{gweon2021inferential}.




\section{Experiment}

\subsection{Participants}

Participants were 256 adults recruited from Prolific who completed the task in a web browser. 12 participants were removed due to technical issues encountered during the experiment, resulting in 244 participants with complete data (average age: 33.8 years, SD = 11.3; 127 male, 103 female, 13 non-binary; educational background distributed across high school, 4-year college, and graduate degrees). The experiment lasted approximately 25 minutes and participants were paid \$14/hr based on this expected completion time. 


\subsection{Stimuli}
In this experiment, participants were tasked with catching a ball launched from a point on a large circle using a rectangular paddle positioned along the outside of the circle (see Figure \ref{fig:stim}).\footnotemark{} 
Participants worked together with an artificial agent ``partner'' who was trying to help them on the task. In each round, their partner suggested a paddle location based on the ball's starting position and participants could either accept their partner's suggestion or adjust the paddle themselves before launching the ball.  

\footnotetext{All code used to run the experiment, as well as code used in analyses below, can be found at: MASKED FOR ANONYMOUS REVIEW.}

Participants were assigned to one of four conditions that manipulated the quality of their partner's suggested paddle locations: an \textit{unreliable} partner, a \textit{reliable} partner, an \textit{improving} partner, and a \textit{worsening} partner. The agent's
suggested paddle location in each trial was an angle $x$ sampled from a von Mises distribution (an approximation of a normal distribution defined over angles around a circle) with mean $\mu$ equal to the ball's final landing angle $\rho$, and variance $\kappa$ set based on the agent's competence level for that condition. The \textit{reliable} agent had a low $\kappa$ value of XX, meaning that the sampled paddle location was almost always close to the ball's true landing location. In contrast, the \textit{unreliable} agent sampled its paddle locations from a high variance distribution with $\kappa = XX$, meaning that while suggestions on some trials were accurate, many were not. The high and low competence $\kappa$ values were chosen to give the agents expected accuracy levels of around 80\% and 20\%, respectively. Meanwhile, the \textit{improving} agent began with a $\kappa$ value equal to the \textit{unreliable} agent's $\kappa$ value, but every 12 trials the $\kappa$ was increased by a fixed amount so that during the final 12 trials, it had a $\kappa$ equal to the \textit{reliable} agent. The \textit{worsening} agent was symmetrical but in the opposite direction.


% FIGURE: Stimuli
\begin{figure}[H]
\begin{center}
\includegraphics[width=\linewidth]{img/stimulus_overview.png}
\end{center}
\caption{A sample experimental trial. Top left: the agent partner moves the paddle to a suggested location. Bottom left: participants are given a chance to either keep their partner's suggested location or adjust it. Top right: after adjusting the agent's suggested paddle location, participants see their partner's original suggestion for comparison. Bottom right: participants are shown the result after launching the ball.} 
\label{fig:stim}
\end{figure}


The task was composed of 96 trials divided into eight blocks of 12. These ``blocks'' were not visible to participants; in each block of trials, the ball appeared at a location sampled from each of the 12 hours on a clock face with some jitter so that participants would not rely on regularity in ball launch locations. The trials were randomized in each block so there was no structured pattern from one launch location to the next. The blocks were also used to update the accuracy of the \textit{improving} and \textit{worsening} agents described above; within each block, they had a stable accuracy that was better (\textit{improving}) or worse (\textit{worsening}) than the previous block. The ball's launching locations for the 96 trials were pre-computed to allow for simulations that calculated the ball's final landing location (they were therefore the same for every participant). However, the agents' suggested paddle locations were sampled during each trial as described above, allowing them to vary across participants. 

Additionally, during each block, a randomly chosen trial was designated as a \textit{critical trial} to measure differences in trust across conditions (participants were not informed of this); rather than sampling the bot's suggested location as described above, the suggested paddle location on critical trials was chosen to have a fixed distance from the ball's estimated landing location. This distance (0.28 radians, or around 16 degrees) was chosen to ensure that the agent's suggestion would not catch the ball, but would be close enough that a trusting participant might accept it. Each participant saw eight critical trials over the course of the experiment. We compare intervention behavior across conditions on these trials to see whether decisions about when to intervene and how much differed with each of the four agents even when the absolute error of the paddle suggestion was the same. 


\subsection{Procedure}
Each trial began with participants' agent partner suggesting a paddle location that would catch the ball; the paddle was shown moving around the circle and a small animation on the right showed the bot ``thinking.'' Once the bot had moved the paddle to its suggested location, participants were given the opportunity to either adjust the paddle's location with the arrow keys or simply keep their partner's suggestion. If participants adjusted the paddle location, the agent's original suggestion was indicated with a gray outline so that participants would be able to see how close the ball landed to the original suggestion on each trial. When participants were happy with the paddle's location, they launched the ball using the spacebar. The path of the ball was animated and participants were shown a message indicating whether they had successfully caught the ball before proceeding to the next trial. 

After completing all 96 trials, participants were given a post-experiment questionnaire containing two parts. First, a series of demographic questions prompted participants for their age, gender identity (optional), level of education, and two covariates which we did not analyze here: number of physics classes taken in their life and hours of video games played in their life. Next, they were asked several questions about their decisions during the experiment. On a slider scale ranging from 0-100\%, participants indicated how often they thought they had intervened on the previous trials and how often they would expect to intervene if they were to play another 96 rounds with this same partner. Finally, they were asked to indicate on a five point Likert scale how much they trusted the agent to catch the ball and how they decided whether to intervene on a given trial (two additional questions asked how much effort they had put into the task and whether they had experienced technical difficulties).



\section{Results}

\subsection{People combine information sources to make intervention decisions}

To understand how participants chose whether to intervene or trust their bot partner, we compare three possible accounts of their decision making. First, it may be that people were entirely trusting of their bot partner, regardless of condition. On this view, participants' own physical intuitions would have played no role in their decision making. A second account takes the opposite perspective; people may have essentially ignored their bot partner's suggestions, simply looking for the best paddle location each round (i.e., if the bot partner's suggestion was accurate, people would accept it and if not, they would intervene to correct it). Finally, a third possible account is that people's behavior was somewhere in the middle of these two. Rather than constantly following their bot partner's suggestion or unilaterally choosing the optimal paddle position each round, people may have relied on a combination of their own physical intuitions and information provided by their bot partner's suggestion to come to a decision about where to place the paddle [CITE]. 

\subsubsection{Participants intervened to improve accuracy.} We start by considering the first hypothesis above, that people merely chose in accordance with their bot partner's suggestions; they may have been entirely trusting of their partner or unable to bring their own intuitive physics understanding to bear on the task, leaving them no other option. If this were true, we would expect intervention rates to be low and overall performance to differ widely across conditions, since the bots varied considerably in how accurate their suggestions were. Figure \ref{fig:rmse} (top) shows the average of each subject's intervention rate (the percent of trials in which they modified the bot's original suggestion) in each trial block. Notably, intervention rates were quite high across the board; even with the \textit{reliable} agent, which was correct on approximately 80\% of trials, participants intervened over 50\% of the time throughout the experiment. Alongside the high overall intervention rates, intervention behavior between conditions reflected the different competence levels of each bot partner and showed a sensitivity to the changing accuracy of the \textit{improving} and \textit{worsening} agents. Thus, far from merely trusting their bot partner's suggestions, participants took an active role in calibrating their interventions to their partner's ability.


But how successful were these interventions? We compare participants' overall performance across conditions by calculating the root mean squared error (RMSE) of each participant's paddle location relative to the ball's final landing location. Figure \ref{fig:rmse} (bottom) shows each participant's average RMSE in degrees across conditions and trial blocks. The upper and lower dashed lines indicate the average RMSE of the \textit{unreliable} and \textit{reliable} agent suggestions, respectively. Initially, human error was high for the \textit{unreliable} and \textit{improving} agent conditions. However, by the end of the experiment, participants in all four conditions showed similarly low error rates. This suggests that, rather than merely deferring to their bot partner, they were able to master the task by intervening in a way that was calibrated to their partner's competence.  Broadly, this suggests that participants were not merely trusting their bot partner throughout the task; in fact, they intervened frequently and in a way that allowed them to achieve similarly low error rates across all four conditions. 


% FIGURE: Subject RMSE by condition, trial block
\begin{figure}[H]
\begin{center}
\includegraphics[width=\linewidth]{img/rmse_intervention_rate.pdf}
\end{center}
\caption{(Top): Average of each participant's paddle intervention rates by trial block. Participants showed high overall intervention rates but differed in ways that were calibrated to their partner's ability. (Bottom): Average of each participant's root mean squared error (RMSE) by trial block. Dashed lines indicate the average RMSE of the \textit{unreliable} (high) and \textit{reliable} (low) agents.}
\label{fig:rmse}
\end{figure}


\begin{figure*}[hbtp]
% \centering
\vspace{-8mm}
\includegraphics[width=\textwidth]{img/error_distributions_clean.pdf} 
\vspace{-4mm}
\caption{Distribution of trial error in each condition, with positive values indicating responses whose error was in the same direction as the bot's suggestion and negative values indicating the opposite. The black dashed line indicates the expected value of 0 if participant error was normally distributed. Color coded vertical lines indicate the empirical mean in each set of trials.} 
\label{fig:error_histograms}
\end{figure*}


\subsubsection{Intervention decisions incorporated bot suggestions.}

Given the high intervention rates across conditions (Figure \ref{fig:rmse}), one account of people's behavior in the task is that they simply relied on their own intuitive physics model to respond. Critically, on this view, their partner's ability is essentially irrelevant. If this were true, then we would expect there to be no impact of the bot's suggestion on people's behavior, e.g., minimal adjustments in the \textit{reliable} condition would simply reflect the fact that on most trials, little adjustment was needed. 

To test this possibility, we examine the distribution of participants' errors on each trial relative to the ball's final landing location and their partner's suggestion. Intuitively, if people were essentially ignoring their partner, we would expect their errors to be roughly normally distributed around the ball's true landing location, just as often to one side as the other, regardless of the bot's paddle suggestion. On the other hand, insofar as people incorporated their partner's suggestion into their decision about where to place the paddle [CITE cue integration], we might expect the bot's suggested paddle location to have an attractive or repellent effect on participants. Figure \ref{fig:error_histograms} shows the distributions of participant error in each condition for the first and last two trial blocks. Critically, these distributions are signed relative to the ball's landing location and the bot's paddle suggestion; an error of 0 (degrees) represents a perfectly accurate paddle placement, while error greater than 0 represents participants placing the paddle away from the ideal catching location \textit{in the direction of the bot's suggestion} and error less than 0 represents participants placing the paddle away from the ideal location \textit{in the opposite direction of the bot's suggestion}. While error is close to normal and clustered near 0 in all four conditions, people's paddle placements were disproportionately closer to their bot partner's suggestion rather than away from it. The dashed lines in Figure \ref{fig:error_histograms} show the empirical average error. Though participants moved closer to the ball's landing location over the course of their experiment, average participant responses were significantly greater than 0 even in the final two blocks, reflecting a stable bias toward the bot's suggested paddle locations (\textit{Reliable}: \textit{t}(56) = 13.90, \textit{p} \textless{0.001}; \textit{Improving}: \textit{t}(64) = 9.61, \textit{p} \textless{0.001}; \textit{Worsening}: \textit{t}(54) = 9.62, \textit{p} \textless{0.001}; \textit{Unreliable}: \textit{t}(66) = 4.89, \textit{p} \textless{0.001}). This suggests that people's decisions about where to place the paddle were not merely an effort to find the best location independent of the bot's suggestion; rather, people showed a systematic anchoring towards the bot's suggestion in where they ultimately placed the paddle.  

Taken together, the results in Figures \ref{fig:rmse} and \ref{fig:error_histograms} suggest that people's decisions about where to place the paddle flexibly integrated multiple sources of information. In other words, they did not show evidence of simply trusting their bot partner regardless of its competence, nor did they simply choose the best move each round without consideration for their partner's suggestion. However, the bot's paddle suggestion on a given trial is not the only source of information that might help participants decide where to ultimately place the paddle. Rather, across repeated interactions, bot partners in each condition offer evidence of their underlying \textit{competence} through the accuracy of their paddle suggestions. Participants can use this information to calibrate \textit{how much} their final paddle locations should be influenced by their partner: in essence, whether their partner is trustworthy. 


\subsection{People relied on past performance to guide interventions}

Since bot partners varied across conditions in how helpful their paddle suggestions were, we hypothesize that participants incorporated this information into their decisions about how closely to follow their partner's suggestions. To do this, participants would have had to maintain an estimate of the overall competence of their partner across repeated trials which could then serve as the basis for deciding how much to be swayed by their partner's suggestion. 

To test this hypothesis, we compare intervention behavior on the eight \textit{critical trials} that each participant completed. If people's responses were merely a result of combining their own estimate of the ball's final location with a consistent offset towards the bot's suggestion, we should not see any difference in intervention behavior on these trials, since the bot's error on critical trials was held constant. However, if people's behavior reflected an underlying judgment about their partner's credibility from previous trials, then we might expect their intervention decisions to differ across the four conditions in line with the differences in bot accuracy. Figure \ref{fig:critical_trials} shows participants' average intervention rates on critical trials (i.e., the proportion of critical trials on which they intervened) and the average intervention magnitude (i.e., how far they moved the paddle from its suggested location) on critical trials in which they intervened. Dashed lines indicate optimal behavior (participants should have intervened by a little over 16 degrees on all critical trials). Overall, participant intervention rates varied significantly across conditions (\textit{F}(3, 240) = 24.34, \textit{p} \textless{0.001}), with follow-up t-tests finding significant differences between all conditions in individual intervention rates. This suggests that people's decisions about whether it was necessary to intervene on critical trials were influenced by the underlying competence of their partner. People displayed a similar pattern in their intervention magnitudes on critical trials in which they intervened. While those paired with an \textit{unreliable} or \textit{improving} partner adjusted the paddle by an amount close to the optimal level, participants whose partner was very accurate (\textit{reliable}) or started out highly accurate (\textit{worsening}) made smaller adjustments on critical trials (individual-level intervention magnitudes are noisy when excluding critical trials in which individuals did not intervene, but the graph at right in Figure \ref{fig:critical_trials} exhibits a clear aggregate pattern). Thus, a complete account of  reasoning on this task suggests that people maintain an underlying assessment of their partner's competence over time and calibrate their decision about whether to intervene and how much based on this assessment.


% FIGURE: Critical trials
\begin{figure}[H]
\begin{center}
\includegraphics[width=\linewidth]{img/critical_trial_intervention_summary.pdf}
\end{center}
\caption{Intervention behavior on \textit{critical trials}. Left: average proportion of critical trials on which participants chose to intervene. The dashed line indicates optimal behavior (critical trials always required intervention to catch the ball). Right: average distance participants intervened on critical trials in which they chose to intervene. The dashed line indicates the optimal intervention distance on these trials.} 
\label{fig:critical_trials}
\end{figure}


\subsection{Past performance and future expectations}

In the previous results, we found evidence that participants' decisions about when to intervene with their bot partner, and how much, involve integrating their own estimates of where the ball will land with an ongoing estimate of their partner's underlying competence that modulates how closely they follow their partner's suggestion. \textit{But how do people generate this estimate?} Do they simply average their partner's performance over the most recent handful of trials? Or do first impressions matter more? To better understand the \textit{basis} for participants' judgments of their partner's competence, we examine responses on the post-experiment survey. First, participants' estimates of how often they had intervened with their partner in the experimental trials were significantly correlated overall (\textit{r} = 0.65, \textit{p} \textless{0.001}), though participants exhibited a tendency to underestimate intervention rates in all conditions, \textit{t}(240) = 3.87, \textit{p} \textless{0.001}. Thus, people's decisions about whether to intervene may have reflected a fairly accurate accounting of past experience. 

But did this enable a calibrated estimate of their partner's underlying ability? Figure \ref{fig:survey} shows the estimated intervention rates from experimental trials paired with participants' responses indicating how much they would \textit{expect} to intervene if they played 96 more rounds with the same partner. The difference in these values across conditions provides some indication of how accurately participants formed future expectations about their partner based on past performance. Responses suggest that people's ability to explicitly forecast their partner's future behavior was limited. Expected intervention rates were significantly lower than estimated past rates for the \textit{worsening} agent (\textit{t}(53) = -5.13, \textit{p} \textless{0.001})---consistent with people having recognized that their partner's performance was deteriorating---but showed a non-significant increase for the \textit{improving} agent, \textit{t}(63) = 1.71, \textit{p} = 0.09. The interaction between these responses for the \textit{improving} and \textit{worsening} conditions was significant (\textit{F}(1, 1) = 4.27, \textit{p} = 0.04), but the lack of discernible difference between expectations for these two agents suggests that people were doing limited extrapolation based on the changes in these bots' accuracies. Meanwhile, forecasted intervention rates remained stable for the \textit{unreliable} agent, as we might expect given its lack of improvement (\textit{t}(66) = -0.1, \textit{p} = 0.92), but showed a significant decrease for the \textit{reliable} agent (\textit{t}(55) = 2.16, \textit{p} = 0.04), perhaps reflecting a growing understanding that little intervention was needed in this condition. These forecasts, far from being idle speculation, may have drawn on participants' underlying trust in their bot partner. The predicted future intervention rates (0-100) were highly negatively correlated with participants' responses on a 5-point Likert scale question which asked how much they trusted their partner to catch the ball on any given trial (\textit{Not at all}, \textit{Slightly}, \textit{Moderately}, \textit{Very}, \textit{Extremely}), \textit{r} = -0.52, \textit{p} \textless{0.001}, exposing the potential role of expectations about future behavior in our trust in others.


% FIGURE: post-survey
\begin{figure}[H]
\begin{center}
\includegraphics[width=\linewidth]{img/survey_intervention_expectations_clean.pdf}
\end{center}
\caption{Participant estimates of how much they intervened with each bot partner compared to how much they would expect to intervene in future rounds with the same partner. Results of post-experiment questionnaire suggest that people form expectations of their bot partner's future performance based on attributions of the bot's competence.} 
\label{fig:survey}
\end{figure}


\section{Discussion}
In this study, we address the question of how people evaluate an artificial agent's \textit{competence} in a novel setting, a key underpinning of forming trust relationships. Participants played a video game in which they positioned a paddle around the outside of a circle to catch a ball launched from different points around the circle. Participants were paired with a bot partner who suggested paddle locations on each trial and participants chose whether to accept or modify their suggestion. Critically, bot partners varied in the accuracy of their suggestions, allowing us to explore the ways in which differences in bot competence impacted people's decision to trust their partner. This work builds on previous research in two central ways. First, we explore human behavior embedded in a rich physical setting, allowing for nuanced judgments of a partner's competence that extend beyond simple assessments such as their win probability or ability to perform a specific task. Second, while prior work has largely focused on evaluating the competence of static agents, our approach in this work addresses a critical aspect of trust relationships with artificial agents and other humans: their ability to learn over time. We probe people's sensitivity to changes in their partner's ability across repeated interactions to better understand how an agent's improving and deteriorating performance impact assessments of competence. 

Our results contain two key findings. First, we show that people's behavior in this task, rather than exclusively trusting their partner's suggestions or simply choosing the best move without regard for their partner, involve a combination of their own intuitive physical judgments and the suggestions of their partner. In other words, people integrate cues from both their own and their partner's reasoning. Second, we show that this process of integrating information from their partner extends beyond a mere bias towards their partner's estimate; instead, people calibrate \textit{how much to defer to their partner} based on the reliability of their partner's suggestions. In other words, their behavior is informed by an ongoing estimate of their partner's competence. We further show that this estimate is not static but rather sensitive to changes in their partner's competence over time when paired with agents that improve and deteriorate. In this way, our findings make headway on better understanding the behavioral underpinnings that allow us to trust an artificial agent across repeated interactions. 

The current results raise a number of questions about trust and evaluations of competence in others. First, how nuanced is our perception of another agent's \textit{learning}? While the current work simply probes this through increasing reliability, there are many ways in which we can detect another person or agent's improvement. Future work should probe our ability to decompose a task into different skills and interpret an agent's errors in light of these skills. Second, while the current experiment focuses on a single task domain, a key component of the trust we place in others is the degree to which we believe their competence \textit{generalizes} to new settings. Physical tasks such as the one in our experiment offer a rich opportunity to explore questions of transfer and generalization. By providing a more thorough account of the ways we judge competence from the behavior of others, the current work can inform future efforts aimed at building more trusting relationships between humans and learning agents as well as the people around us.




% \section{Acknowledgments}
% In the \textbf{initial submission}, please \textbf{do not include
%   acknowledgements}, to preserve anonymity.  In the \textbf{final submission},
% place acknowledgments (including funding information) in a section \textbf{at
% the end of the paper}.





\bibliographystyle{apacite}
\setlength{\bibleftmargin}{.125in}
\setlength{\bibindent}{-\bibleftmargin}
\typeout{} % erikb for some reason this line is necessary...
\bibliography{references}


\end{document}
